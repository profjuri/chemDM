#!/usr/bin/env python3

import os
import sys
import time
import pandas as pd
import torch
import yaml
from torch import nn
from torch.optim.lr_scheduler import ReduceLROnPlateau

from functions import selfies_to_one_hot, get_selfie_and_smiles_encodings_for_dataset
from functions_sub import _make_dir, get_free_memory

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")



def save_models(encoder, decoder, epoch, settings, encoding_alphabet, perm_df):

    '''This function saves the encoder and decoder parameters'''

    '''Arguments:
                    encoder: the encoder object (VAEEncoder object)
                    decoder: the decoder object (VAEDecoder object)
                    epoch: the current epoch (int)
                    settings: settings defined by the .yml file (dict)
                    encoding_alphabet: the alphabet generated by the function get_selfie_and_smiles_encodings_for_dataset (list)'''

    save_path = settings['data']['save_path']
    out_dir = str(save_path) + '/{}'.format(epoch) 
    _make_dir(out_dir)

    torch.save(encoder.state_dict(), '{}/E.pt'.format(out_dir))
    torch.save(decoder.state_dict(), '{}/D.pt'.format(out_dir))

    settings_folder = str(save_path) + '/settings'
    log_folder = settings_folder
    log_filename = 'settings.yml'

    if not os.path.exists(settings_folder):
        os.makedirs(settings_folder)

        log_filepath = os.path.join(log_folder, log_filename)
        data = {**settings, 'alphabet': encoding_alphabet}

        with open(log_filepath, 'w') as file:
            yaml.dump(data, file)

        perm_df.to_csv(settings_folder + '/PERM_IDX.csv', index=False)

        


def save_models_epoch_loss(epoch, loss, recon_loss, val_loss, val_recon, kld_loss, settings):

    '''This function saves the epoch, total training loss, trainin reconstruction loss, training kld loss and the total validation loss to a .txt file'''

    '''Arguments:
                    epoch: the epoch currently being saved (int)
                    loss: the total training loss (float)
                    recon_loss: the training reconstruction loss (float)
                    val_loss: the total validation loss (float)
                    val_recon: the total validation reconstruction rate, i.e., the ratio of correct SELFIES characters predicted from reconstruction (float)
                    kld_loss: the training kld loss (float)
                    settings: settings defined by the .yml file (dict)'''
    
    save_path = settings['data']['save_path']
    out_dir = str(save_path) + '/settings'
    log_folder = out_dir
    log_filename = 'epoch_loss.txt'
    log_filepath = os.path.join(log_folder, log_filename)

    if not os.path.exists(log_folder):
        os.makedirs(log_folder)

    file_exists = os.path.isfile(log_filepath)
    ratio = recon_loss/kld_loss
    
    with open(log_filepath, 'a') as file:
        if not file_exists:
            file.write("epoch,loss,recon_loss,kld_loss,ratio,val_loss,val_recon\n")
        file.write(f'{epoch},{loss},{recon_loss},{kld_loss},{ratio},{val_loss},{val_recon}\n')





class VAEEncoder(nn.Module):

    def __init__(self, in_dimension, gru_stack_size, gru_neurons_num,
                 latent_dimension):

        '''Recurrent layers to encode molecule to latent space'''

        '''Arguments:
                        in_dimension: the number of elements in each one hot array (int)
                        gru_stack_size: the number of gru_stacks (int)
                        gru_neurons_num: the number of neurons (int)
                        latent_dimension: the size of the latent space (int)'''
        
        super(VAEEncoder, self).__init__()
        self.latent_dimension = latent_dimension
        self.gru_stack_size = gru_stack_size
        self.gru_neurons_num = gru_neurons_num

        self.encode_RNN = nn.GRU(
            input_size=in_dimension,
            hidden_size=gru_neurons_num,
            num_layers=gru_stack_size,
            batch_first=False)

        self.encode_FC_mu = nn.Sequential(
            nn.Linear(gru_neurons_num, latent_dimension),
        )

        self.encode_FC_log_var = nn.Sequential(
            nn.Linear(gru_neurons_num, latent_dimension),
        )

    @staticmethod
    def reparameterize(mu, log_var):

        '''Reparameterization trick'''

        '''Arguments:
                        mu: the mean latent vector (Pytorch float.32 tensor)
                        log_var: the natural logarithm of the variance tensor of each latent vector (Pytorch float.32 tensor)'''
        
        '''Outputs:
                        z: a latent vector modified by some distribution defined by the standard deviation (Pytorch float.32 tensor)'''

        std = torch.exp(0.5 * log_var)
        eps = torch.randn_like(std)
        return eps.mul(std).add_(mu)

    def forward(self, x, hidden=None):

        '''Pass through the Encoder RNN'''

        '''Arguments:
                        x: The feed forward object, i.e., the transformed one hot vector (Pytorch float.32 tensor)'''
        
        '''Outputs:
                        z: a latent vector modified by some distribution defined by the standard deviation (Pytorch float.32 tensor)
                        mu: the mean latent vector (Pytorch float.32 tensor)
                        log_var: the natural logarithm of the variance tensor of each latent vector (Pytorch float.32 tensor)'''


        if hidden is None:
            hidden = self.init_hidden(x.size(1))

        rnn_output, hidden = self.encode_RNN(x, hidden)
        rnn_output_last = rnn_output[-1, :, :]

        mu = self.encode_FC_mu(rnn_output_last)
        log_var = self.encode_FC_log_var(rnn_output_last)
        z = self.reparameterize(mu, log_var)

        return z, mu, log_var

    def init_hidden(self, batch_size=1):
                
        '''Hidden layer initalisation'''

        weight = next(self.parameters())
        return weight.new_zeros(self.gru_stack_size, batch_size,
                                self.gru_neurons_num)


class VAEDecoder(nn.Module):

    def __init__(self, latent_dimension, gru_stack_size, gru_neurons_num,
                 out_dimension, seq_len):
        
        '''Recurrent layers to decode latent vectors to one hot vectors'''

        '''Arguments:
                        latent_dimension: the size of the latent space (int)
                        gru_stack_size: the number of gru layers (Int)
                        gru_neurons_num: the number of neurons in each layer (int)
                        out_dimension: the length of the selfies_alphabet (int)
                        seq_len: the number of timesteps wanted. This is meant to correspond to the largest_molecule_length (int)'''
        

        super(VAEDecoder, self).__init__()
        self.latent_dimension = latent_dimension
        self.gru_stack_size = gru_stack_size
        self.gru_neurons_num = gru_neurons_num
        self.seq_len = seq_len

        self.decode_RNN = nn.GRU(
            input_size=latent_dimension,
            hidden_size=gru_neurons_num,
            num_layers=gru_stack_size,
            batch_first=True)

        self.decode_FC = nn.Sequential(
            nn.Linear(gru_neurons_num, out_dimension),
        )

    def init_hidden(self, batch_size=1):

        '''Initalises the hidden layer'''

        weight = next(self.parameters())
        return weight.new_zeros(self.gru_stack_size, batch_size,
                                self.gru_neurons_num)

    def forward(self, z):

        '''A forward pass throught the entire model.'''

        '''Arguments:
                        z: the latent vector (Pytorch float.32 tensor)'''
        
        '''Outputs:
                        decoded: the decoded output, should be a one hot vector (Pytorch float.32 tensor)'''

        hidden = self.init_hidden(batch_size=z.shape[0]).to(device)
        z = z.unsqueeze(1).repeat(1,self.seq_len,1)

        l1, hidden = self.decode_RNN(z, hidden)
        decoded = self.decode_FC(l1)

        return decoded




def train_model(vae_encoder, vae_decoder, num_epochs, batch_size,
                lr_enc, lr_dec, KLD_alpha,
                largest_molecule_len, encoding_alphabet, settings, encoding_list, perm_df):
    
    '''Train the Variational Auto-Encoder'''

    '''Arguments:
                    vae_encoder: the encoder object (VAEEncoder object)
                    vae_decoder: the decoder object (VAEDecoder object)
                    num_epochs: the desired number of epochs (int)
                    batch_size: the batch size (int)
                    lr_enc: the learning rate for the encoder training (float)
                    lr_dec: the learning rate for the decoder training (float)
                    KLD_alpha: the weight you apply to the KLD term (float)
                    largest_molecule_len: the maximum molecule length in terms of SELFIES characters (int)
                    encoding_alphabet: the alphabet generated by the function get_selfie_and_smiles_encodings_for_dataset (list)
                    settings: settings defined by the yml file (dict)
                    encoding_list: a list containing the SELFIES (list)''' 

    num_chunks = 25
    chunk_size = int(len(encoding_list)/num_chunks)    

    optimizer_encoder = torch.optim.Adam(vae_encoder.parameters(), lr=lr_enc)
    optimizer_decoder = torch.optim.Adam(vae_decoder.parameters(), lr=lr_dec)
    scheduler = ReduceLROnPlateau(optimizer_encoder, mode='min', factor=0.5, patience=10, verbose=True)
    scheduler2 = ReduceLROnPlateau(optimizer_decoder, mode='min', factor=0.5, patience=10, verbose=True)

    total_mem_req = (4*(len(encoding_list)*largest_molecule_len*len(encoding_alphabet)))
    free_memory = get_free_memory(device)
    memory_ratio = total_mem_req/free_memory + 1
    num_clusters = int(memory_ratio)

    train_chunk = int(0.8 * num_chunks)
    valid_chunk = int(0.2 * num_chunks)
    time_0 = time.time()

    for epoch in range(num_epochs):

        print('Time for epoch', epoch, ':', time.time() - time_0)
        time_0 = time.time()
        
        for chunk_iteration in range(train_chunk):
            

            start_idx = int(chunk_iteration * chunk_size)
            stop_idx = int((chunk_iteration + 1) * chunk_size)
            sub_encoding = encoding_list[start_idx: stop_idx]
            data_train = selfies_to_one_hot(sub_encoding, encoding_alphabet, largest_molecule_len)
            cluster_size = len(data_train) / num_clusters
            if num_clusters*cluster_size < len(data_train):
                num_clusters = num_clusters+1

            for cluster_iteration in range(num_clusters):

                start_idx = int(cluster_iteration * cluster_size)
                stop_idx = int((cluster_iteration + 1) * cluster_size)

                cluster_data = data_train[start_idx: stop_idx].to(torch.float32).to(device)

                num_batches_train = int(len(cluster_data) / batch_size)
                if num_batches_train*batch_size < len(cluster_data):
                    num_batches_train = num_batches_train +1

                for batch_iteration in range(num_batches_train):

                    start_idx = batch_iteration * batch_size
                    stop_idx = (batch_iteration + 1) * batch_size
                    batch = cluster_data[start_idx: stop_idx]

                    inp_flat_one_hot = batch.flatten(start_dim=1)
                    inp_flat_one_hot = inp_flat_one_hot.unsqueeze(0)

                    latent_points, mus, log_vars = vae_encoder(inp_flat_one_hot)
                    out_one_hot = vae_decoder(latent_points)                    
            
                    loss, recon_loss, kld_loss = compute_elbo(batch, out_one_hot, mus, log_vars, KLD_alpha)
                    loss = loss 

                    optimizer_encoder.zero_grad()
                    optimizer_decoder.zero_grad()
                    loss.backward(retain_graph=True)
            
                    nn.utils.clip_grad_norm_(vae_decoder.parameters(), 10.0)
                    optimizer_encoder.step()
                    optimizer_decoder.step()

        scheduler.step(loss)
        scheduler2.step(loss)

        if epoch % 10 == 0:
            save_models(vae_encoder, vae_decoder, epoch, settings, encoding_alphabet, perm_df)

        vae_encoder.eval()
        vae_decoder.eval()
        
        total_valid_recon_loss = torch.tensor([0]).to(device)
        total_recon = torch.tensor([0]).to(device)

        with torch.no_grad():
            for chunk_iteration in range(valid_chunk):


                start_idx = int((train_chunk+chunk_iteration) * chunk_size)
                stop_idx = int((train_chunk+chunk_iteration+1) * chunk_size)
                sub_encoding = encoding_list[start_idx: stop_idx]
                data_valid = selfies_to_one_hot(sub_encoding, encoding_alphabet, largest_molecule_len)
                cluster_size = len(data_valid) / num_clusters
                if num_clusters*cluster_size < len(data_valid):
                    num_clusters = num_clusters+1

                one_hot_list = []


                for cluster_iteration in range(num_clusters):

                    start_idx = int(cluster_iteration * cluster_size)
                    stop_idx = int((cluster_iteration + 1) * cluster_size)
                    cluster_data = data_valid[start_idx: stop_idx].to(torch.float32).to(device)
                    num_batches_valid = int(len(cluster_data) / batch_size)
                    if num_batches_valid*batch_size < len(cluster_data):
                        num_batches_valid = num_batches_valid +1



                    for batch_iteration in range(num_batches_valid):

                        start_idx = batch_iteration * batch_size
                        stop_idx = (batch_iteration + 1) * batch_size
                        batch = cluster_data[start_idx: stop_idx]

                        inp_flat_one_hot = batch.flatten(start_dim=1)
                        inp_flat_one_hot = inp_flat_one_hot.unsqueeze(0)
                        latent_points, mus, log_vars = vae_encoder(inp_flat_one_hot)
                        
                        out_one_hot = vae_decoder(latent_points)
                        one_hot_list.append(out_one_hot.to('cpu'))
                        


                one_hot_tensor = torch.cat(one_hot_list)
                val_loss, val_recon = validation_loss(data_valid, one_hot_tensor, settings)
                total_valid_recon_loss = total_valid_recon_loss + val_loss/valid_chunk
                total_recon = total_recon + val_recon/valid_chunk


        vae_encoder.train()
        vae_decoder.train()

        save_models_epoch_loss(epoch, loss.item(), recon_loss, total_valid_recon_loss.item(), total_recon.item(), kld_loss, settings)





def compute_elbo(x, x_hat, mus, log_vars, KLD_alpha):

    '''Compute the total loss of the VAE (encoder and decoder are defined by one shared loss)'''

    '''Arguments:
                    x: the batch/ the one hot vector object defined by the batch size (Pytorch float.32 tensor)
                    x_hat: the one hot vector object after it has been fed through the encoder and decoder (Pytorch float.32 tensor)
                    mus: the mean latent vector object (Pytorch float.32 tensor)
                    log_vars: the natural logarithm of the variance tensor of each latent vector (Pytorch float.32 tensor)
                    KLD_alpha: the weight you apply to the KLD term (Pytorch float.32 tensor)'''
    
    '''Outputs:
                    total_loss: the total training loss (Pytorch float.32 tensor)
                    recon_loss: the reconstruciton training loss (Pytorch float.32 tensor)
                    KLD loss: the KLD training loss (Pytorch float.32 tensor)'''

    inp = x_hat.reshape(-1, x_hat.shape[2])
    target = x.reshape(-1, x.shape[2]).argmax(1)

    criterion = torch.nn.CrossEntropyLoss()
    recon_loss = criterion(inp, target)
    kld = -0.5 * torch.mean(1. + log_vars - mus.pow(2) - log_vars.exp())

    total_loss = recon_loss + (KLD_alpha * kld) 

    return total_loss, recon_loss, KLD_alpha * kld

def validation_loss(x, x_hat, settings):

    '''Gives the reconstruction loss of the validation set'''

    '''Arguments:
                    x: the batch/ the one hot vector object defined by the batch size (Pytorch float.32 tensor)
                    x_hat: the one hot vector object after it has been fed through the encoder and decoder (Pytorch float.32 tensor)
                    settings: settings defined by the corresponding .yml file (dict)'''

    '''Outputs:
                    avg_recon_loss: the validation reconstruct loss (Pytorch float.32 tensor)
                    avg_total_recon: the total validation reconstruction rate, i.e., the ratio of correct SELFIES characters predicted from reconstruction (Pytorch float.32 tensor)'''
    
    batch_size = settings['data']['batch_size']
    
    recon_loss = torch.tensor([0]).to(device)
    total_recon = torch.tensor([0]).to(device)

    total_mem_req = 3*(4*(x.shape[0]*x.shape[1]*x.shape[2]) + 4*(x_hat.shape[0]*x_hat.shape[1]*x.shape[2]))
    free_memory = get_free_memory(device)
    memory_ratio = total_mem_req/free_memory + 1

    num_clusters = int(memory_ratio)
    cluster_size = len(x) / num_clusters
    if num_clusters*cluster_size < len(x):
        num_clusters = num_clusters+1

    num_batches = 0


    for cluster_iteration in range(num_clusters):

        start_idx = int(cluster_iteration * cluster_size)
        stop_idx = int((cluster_iteration + 1) * cluster_size)

        cluster_x = x[start_idx: stop_idx].to(device)
        cluster_x_hat = x_hat[start_idx: stop_idx].to(device)

        num_batches_train = int(cluster_x.shape[0] / batch_size)
        if num_batches_train*batch_size < len(cluster_x):
            num_batches_train = num_batches_train +1
        num_batches = num_batches + num_batches_train

        for batch_iteration in range(num_batches_train):

            start_idx = batch_iteration * batch_size
            stop_idx = (batch_iteration + 1) * batch_size
            sub_x = cluster_x[start_idx: stop_idx]
            sub_x_hat = cluster_x_hat[start_idx: stop_idx]


            inp = sub_x_hat.reshape(-1, x_hat.shape[2])
            target = sub_x.reshape(-1, x.shape[2]).argmax(1)


            criterion = torch.nn.CrossEntropyLoss()
            sub_loss = criterion(inp, target)
            recon_loss = recon_loss + sub_loss


            inp_hat = sub_x_hat.reshape(-1, sub_x_hat.shape[2]).argmax(1)
            sub_recon = torch.sum((inp_hat==target)*1)/inp_hat.shape[0]

            total_recon = total_recon + sub_recon


    avg_recon_loss = recon_loss/num_batches
    avg_total_recon = total_recon/num_batches


    return avg_recon_loss, avg_total_recon


def data_init(settings):

    '''Data initialisation'''

    '''Arguments:
                    settings: settings defined by the corresponding .yml file (dict)'''
    
    '''Outputs:
                    largest_molecule_len: the maximum length of the molecule encodings (int)
                    len_alphabet: the length of the alphabet used (int)
                    len_max_mol_one_hot: the number of elements in each one hot (int)
                    encoding_alphabet: the alphabet used (list)
                    encoding_list: a list containing the SELFIES (list): '''

    file_name_smiles = settings['data']['smiles_file']
    full_alphabet_set = set(settings['data']['full_alphabet_set'])
    torch_seed = settings['data']['torch_seed']

    encoding_list, encoding_alphabet, largest_molecule_len = get_selfie_and_smiles_encodings_for_dataset(file_name_smiles)
        
    for letter in full_alphabet_set:
        if letter not in encoding_alphabet:
            encoding_alphabet.append(letter)
    encoding_alphabet.append('.')

    torch.manual_seed(torch_seed)
    rand_perms = torch.randperm(len(encoding_list))
    encoding_list = [encoding_list[x] for x in rand_perms]
    perm_df = pd.DataFrame({'PERM_IDX': rand_perms})

    return largest_molecule_len, len(encoding_alphabet), largest_molecule_len*len(encoding_alphabet), encoding_alphabet, encoding_list, perm_df


def main():
    if os.path.exists("vae_settings.yml"):
        settings = yaml.safe_load(open("vae_settings.yml", "r"))
    else:
        print("Expected a file settings.yml but didn't find it.")
        print(sys.path)
        return

    data_parameters = settings['data']
    batch_size = data_parameters['batch_size']
    encoder_parameter = settings['encoder']
    decoder_parameter = settings['decoder']
    training_parameters = settings['training']
    

    torch.cuda.empty_cache()
    largest_molecule_len, len_alphabet, len_max_mol_one_hot, encoding_alphabet, encoding_list, perm_df = data_init(settings)
    vae_encoder = VAEEncoder(in_dimension=len_max_mol_one_hot, **encoder_parameter).to(device)
    vae_decoder = VAEDecoder(**decoder_parameter, out_dimension=len_alphabet, seq_len=largest_molecule_len).to(device)


    print("start training")
    train_model(vae_encoder=vae_encoder,
                vae_decoder=vae_decoder,
                **training_parameters,
                batch_size=batch_size,
                encoding_alphabet=encoding_alphabet,
                largest_molecule_len=largest_molecule_len,
                settings=settings,
                encoding_list=encoding_list,
                perm_df = perm_df
                )



if __name__ == '__main__':
    try:
        main()
    except AttributeError:
        _, error_message, _ = sys.exc_info()
        print(error_message)
