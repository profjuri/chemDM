{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import selfies\n",
    "import glob\n",
    "import torch\n",
    "import chemistry_vae_selfies\n",
    "import data_loader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from random import sample\n",
    "import properties_searcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_properties = []\n",
    "\n",
    "my_file = pd.read_csv('./datasets/PropsQM9/listprops.csv', index_col=None)##The file you want to train on, should contain SMILES reps, latent space reps and properties\n",
    "my_file.dropna()\n",
    "\n",
    "\n",
    "for i in range(len(my_file)):\n",
    "\n",
    "    DipoleIn = my_file['dipole_moment'][i]\n",
    "    GapIn = my_file['energy_gap'][i]\n",
    "    SMILESCodeIn = my_file['smiles'][i]\n",
    "    list_of_properties.append([SMILESCodeIn, GapIn, DipoleIn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Translating SMILES to SELFIES...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished translating SMILES to SELFIES.\n",
      "selfies aplhabet: ['[=Branch2]', '[C]', '[Ring1]', '[O]', '[=Branch1]', '[#Branch1]', '[Branch1]', '[=O]', '[=C]', '[nop]', '[N]', '[F]', '[#Branch2]', '[Ring2]', '[Branch2]', '[=N]', '[#N]', '[#C]']\n",
      "smiles aplhabet: ['O', ')', '#', 'F', 'N', '=', '3', '2', '(', '4', '1', '5', 'C', ' ']\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "folder_path = \"./datasets/\"\n",
    "file_name = \"SelectedSMILES_QM9.txt\"\n",
    "\n",
    "full_path = folder_path + file_name\n",
    "\n",
    "selfies_list, selfies_alphabet, largest_selfies_len, smiles_list, smiles_alphabet, largest_smiles_len = chemistry_vae_selfies.get_selfie_and_smiles_encodings_for_dataset(full_path)\n",
    "\n",
    "selfies_alphabet = ['[#Branch2]', '[Ring2]', '[Branch2]', '[=Branch2]', '[O]', '[=O]', '[=C]', '[=N]', '[#Branch1]', '[=Branch1]', '[nop]', '[N]', '[Branch1]', '[F]', '[#C]', '[#N]', '[Ring1]', '[C]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define source file location\n",
    "file_to_load =  \"./saved_models_RNN/\"\n",
    "# training file name encoder\n",
    "training_file_nameE = \"300/E\"\n",
    "# training file name decoder\n",
    "training_file_nameD = \"300/D\"\n",
    "# load data\n",
    "#load_data_trained = file_to_load + training_file_nameE\n",
    "# Alphabet has 18 letters, largest molecule is 21 letters. (build this as an output function later ... )\n",
    "largest_selfies_len_dataset = largest_selfies_len\n",
    "largest_smiles_len_dataset = largest_smiles_len\n",
    "\n",
    "#in_dimension = len(selfies_alphabet)*largest_selfies_len\n",
    "in_dimension = len(smiles_alphabet)*largest_smiles_len\n",
    "\n",
    "\n",
    "\n",
    "# load the trained encoder\n",
    "vae_encoder = torch.load(file_to_load + training_file_nameE) #, map_location=torch.device(device=\"cpu\"))\n",
    "#print(vae_encoder)\n",
    "\n",
    "# load the trained decoder\n",
    "vae_decoder = torch.load(file_to_load + training_file_nameD) #, map_location=torch.device(device=\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful functions\n",
    "\n",
    "def translate_selfie(sequence):  \n",
    "\n",
    "        SELFIESGenerated = \"\"\n",
    "\n",
    "        for i in range(len(sequence)):\n",
    "                SELFIESGenerated = SELFIESGenerated + selfies_alphabet[sequence[i]]\n",
    "        return SELFIESGenerated\n",
    "\n",
    "def translate_smile(sequence):  \n",
    "\n",
    "        SELFIESGenerated = \"\"\n",
    "\n",
    "        for i in range(len(sequence)):\n",
    "                SELFIESGenerated = SELFIESGenerated + smiles_alphabet[sequence[i]]\n",
    "        return SELFIESGenerated\n",
    "        \n",
    "def create_onehot_instance(selfie_input,largest_selfies_len,selfies_alphabet_in):\n",
    "\n",
    "    inttest_hot, arraytest_hot = data_loader.selfies_to_hot(selfie_input,largest_selfies_len, selfies_alphabet_in)\n",
    "    x = torch.from_numpy(arraytest_hot).flatten().float().unsqueeze(0)\n",
    "    return x\n",
    "\n",
    "def create_onehot_instance_smiles(smile_input,largest_smiles_len,smiles_alphabet_in):\n",
    "\n",
    "    inttest_hot, arraytest_hot = data_loader.smile_to_hot(smile_input,largest_smiles_len, smiles_alphabet_in)\n",
    "    x = torch.from_numpy(arraytest_hot).flatten().float().unsqueeze(0)\n",
    "    return x\n",
    "\n",
    "def create_latent_space_vector(selfie_input,largest_selfies_len,selfies_alphabet_in):\n",
    "\n",
    "    x = create_onehot_instance(selfie_input,largest_selfies_len,selfies_alphabet_in)\n",
    "\n",
    "    z =set()\n",
    "    vae_encoder.eval()\n",
    "    vae_decoder.eval()\n",
    "    z, mu, log_var = vae_encoder(x)\n",
    "\n",
    "    return z.unsqueeze(0)\n",
    "\n",
    "def create_latent_space_vector_smiles(smile_input,largest_smiles_len,smiles_alphabet_in):\n",
    "\n",
    "    x = create_onehot_instance_smiles(smile_input,largest_smiles_len,smiles_alphabet_in)\n",
    "\n",
    "    z =set()\n",
    "    vae_encoder.eval()\n",
    "    vae_decoder.eval()\n",
    "    z, mu, log_var = vae_encoder(x)\n",
    "\n",
    "    return z.unsqueeze(0)\n",
    "\n",
    "def create_random_latent_space_vector(largest_selfies_len,selfies_alphabet_in):\n",
    "\n",
    "    # Random input tensor for tests\n",
    "    in_dimension_input = largest_selfies_len*len(selfies_alphabet_in)\n",
    "    x = torch.randn(in_dimension_input).unsqueeze(0)\n",
    "\n",
    "    z =set()\n",
    "    vae_encoder.eval()\n",
    "    vae_decoder.eval()\n",
    "    z, mu, log_var = vae_encoder(x)\n",
    "\n",
    "    return z.unsqueeze(0)\n",
    "\n",
    "def decode_from_latentspace(latent_point_in, largest_selfies_len_in, selfies_alphabet_len, method):\n",
    "\n",
    "        #one_hot_dimension = torch.zeros(selfies_alphabet_len,largest_selfies_len_in)\n",
    "        #out_one_hot = torch.zeros_like(one_hot_dimension, device=device)\n",
    "\n",
    "        vae_decoder.eval()\n",
    "        vae_encoder.eval()\n",
    "\n",
    "        sequence = []\n",
    "\n",
    "        hidden = vae_decoder.init_hidden(batch_size=1)\n",
    "\n",
    "         \n",
    "        for seq_index in range(largest_selfies_len_in):\n",
    "                out_one_hot_line, hidden = vae_decoder(latent_point_in, hidden)\n",
    "\n",
    "                if method == 0:\n",
    "                        sequence.append(out_one_hot_line.argmax())\n",
    "\n",
    "                elif method ==1:\n",
    "                        # Apply softmax and sample from the distribution to get the next token\n",
    "                        softmax = torch.nn.Softmax(dim=2)\n",
    "                        probabilities = softmax(out_one_hot_line)\n",
    "                        categorical_dist = dist.Categorical(probabilities)\n",
    "                        sample = categorical_dist.sample()\n",
    "                        sequence.append(sample)\n",
    "\n",
    "                else:\n",
    "                        print(\"method is 0 for argmax or 1 for stat sampling\")\n",
    "        \n",
    "        \n",
    "        return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n"
     ]
    }
   ],
   "source": [
    "\n",
    "energy_gaps = []\n",
    "input_dim = 25\n",
    "\n",
    "\n",
    "smiles_rep, latent_rep, props_used = properties_searcher.filter(list_of_properties, largest_selfies_len_dataset, selfies_alphabet, vae_encoder, vae_decoder)\n",
    "\n",
    "\n",
    "latent_space_vectors_valid = [item[0].detach().squeeze(0) for item in latent_rep]\n",
    "properties_training = [torch.tensor(property_vector) for property_vector in props_used]  # this will convert lists to tensors\n",
    "\n",
    "\n",
    "for i in range(len(properties_training)):\n",
    "    energy_gaps.append(props_used[i][0])\n",
    "\n",
    "\n",
    "train_size = round(len(latent_space_vectors_valid)*0.8)\n",
    "\n",
    "latents_train = latent_space_vectors_valid[:train_size]\n",
    "latents_test = latent_space_vectors_valid[train_size:]\n",
    "\n",
    "props_train = energy_gaps[:train_size]\n",
    "props_test = energy_gaps[train_size:]\n",
    "\n",
    "\n",
    "train_latent_space_tensor, test_latent_space_tensor, train_properties_tensor, test_properties_tensor = properties_searcher.make_tensors(latents_train, latents_test, props_train, props_test)\n",
    "\n",
    "\n",
    "train_latent_space_tensor = train_latent_space_tensor.view(-1, input_dim)\n",
    "test_latent_space_tensor = test_latent_space_tensor.view(-1, input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('../../property_hyperparams/0.7329389642767317')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    test_predictions = model(test_latent_space_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction: tensor(0.2563) actual: tensor(0.2563)\n",
      "prediction: tensor(0.2584) actual: tensor(0.2584)\n",
      "prediction: tensor(0.2548) actual: tensor(0.2548)\n",
      "prediction: tensor(0.2509) actual: tensor(0.2509)\n",
      "prediction: tensor(0.3246) actual: tensor(0.3246)\n",
      "prediction: tensor(0.3034) actual: tensor(0.3034)\n",
      "prediction: tensor(0.3109) actual: tensor(0.3109)\n",
      "prediction: tensor(0.2167) actual: tensor(0.2167)\n",
      "prediction: tensor(0.2435) actual: tensor(0.2435)\n",
      "prediction: tensor(0.3223) actual: tensor(0.3223)\n",
      "prediction: tensor(0.2755) actual: tensor(0.2755)\n",
      "prediction: tensor(0.1739) actual: tensor(0.1739)\n",
      "prediction: tensor(0.1982) actual: tensor(0.1982)\n",
      "prediction: tensor(0.2226) actual: tensor(0.2226)\n",
      "prediction: tensor(0.2587) actual: tensor(0.2587)\n",
      "prediction: tensor(0.2968) actual: tensor(0.2968)\n",
      "prediction: tensor(0.2130) actual: tensor(0.2130)\n",
      "prediction: tensor(0.3330) actual: tensor(0.3330)\n",
      "prediction: tensor(0.2413) actual: tensor(0.2413)\n",
      "prediction: tensor(0.2362) actual: tensor(0.2362)\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    ran_num = random.randint(0,len(test_properties_tensor))\n",
    "\n",
    "    print('prediction:',test_properties_tensor[ran_num], 'actual:', test_properties_tensor[ran_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1252, -0.0807,  0.3421, -0.0667,  0.4076,  1.1246, -0.0194, -0.7363,\n",
       "        -0.7846,  0.8017,  0.4254,  0.3517, -1.4349,  0.1400,  0.2409,  0.7326,\n",
       "        -0.5615, -0.5935, -1.7920,  0.3106, -0.2597,  0.1054, -0.1587, -0.3621,\n",
       "        -1.0563])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_latent_space_tensor[sample_point]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23528\n"
     ]
    }
   ],
   "source": [
    "sample_point = random.randint(0,len(test_properties_tensor))\n",
    "print(sample_point)\n",
    "sampled_latent_prediction = []\n",
    "\n",
    "for i in range(10000):\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        sampled_latent_prediction.append(model(test_latent_space_tensor[sample_point]).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_preds = sampled_latent_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3014])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average predicted value: 0.22271709347963334 Standard deviation: +/- 0.0053826940940991915\n",
      "Actual value: 0.20100000500679016\n"
     ]
    }
   ],
   "source": [
    "print(\"Average predicted value:\", np.mean(list_of_preds), \"Standard deviation: +/-\", np.std(list_of_preds))\n",
    "print(\"Actual value:\", test_properties_tensor[sample_point].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_preds = []\n",
    "\n",
    "for i in range(len(list_of_preds)):\n",
    "    list_preds.append(list_of_preds[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  1.,   0.,   0.,   0.,   4.,   8.,   4.,   4.,   5.,  15.,  13.,\n",
       "         34.,  28.,  61.,  76.,  82., 105., 141., 170., 192., 245., 310.,\n",
       "        379., 413., 472., 507., 557., 577., 623., 620., 615., 573., 594.,\n",
       "        501., 458., 396., 301., 250., 186., 145., 115.,  86.,  46.,  31.,\n",
       "         30.,  11.,   7.,   5.,   2.,   2.]),\n",
       " array([0.19893987, 0.19976939, 0.2005989 , 0.20142841, 0.20225792,\n",
       "        0.20308743, 0.20391694, 0.20474645, 0.20557596, 0.20640547,\n",
       "        0.20723498, 0.20806449, 0.208894  , 0.20972351, 0.21055302,\n",
       "        0.21138254, 0.21221205, 0.21304156, 0.21387107, 0.21470058,\n",
       "        0.21553009, 0.2163596 , 0.21718911, 0.21801862, 0.21884813,\n",
       "        0.21967764, 0.22050715, 0.22133666, 0.22216617, 0.22299568,\n",
       "        0.2238252 , 0.22465471, 0.22548422, 0.22631373, 0.22714324,\n",
       "        0.22797275, 0.22880226, 0.22963177, 0.23046128, 0.23129079,\n",
       "        0.2321203 , 0.23294981, 0.23377932, 0.23460883, 0.23543835,\n",
       "        0.23626786, 0.23709737, 0.23792688, 0.23875639, 0.2395859 ,\n",
       "        0.24041541]),\n",
       " <BarContainer object of 50 artists>)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(list_preds, bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
