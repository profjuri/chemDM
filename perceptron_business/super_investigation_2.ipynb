{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import properties_searcher\n",
    "import pandas as pd\n",
    "import torch\n",
    "import chemistry_vae_selfies\n",
    "import importlib\n",
    "import math\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_properties = []\n",
    "\n",
    "input_dim = 25 ### Size of the latent space\n",
    "num_of_cycles = 1000### The number of hyperparameter searches you want to do, change accordingly\n",
    "counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_file = pd.read_csv('./datasets/PropsQM9/listprops.csv', index_col=None)##The file you want to train on, should contain SMILES reps, latent space reps and properties\n",
    "my_file.dropna()\n",
    "\n",
    "\n",
    "for i in range(len(my_file)):\n",
    "\n",
    "    DipoleIn = my_file['dipole_moment'][i]\n",
    "    GapIn = my_file['energy_gap'][i]\n",
    "    SMILESCodeIn = my_file['smiles'][i]\n",
    "    list_of_properties.append([SMILESCodeIn, GapIn, DipoleIn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Translating SMILES to SELFIES...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished translating SMILES to SELFIES.\n",
      "selfies aplhabet: ['[#Branch2]', '[=C]', '[#C]', '[O]', '[Branch2]', '[#N]', '[#Branch1]', '[nop]', '[N]', '[=Branch1]', '[Ring1]', '[Ring2]', '[=O]', '[F]', '[C]', '[=N]', '[=Branch2]', '[Branch1]']\n",
      "smiles aplhabet: ['4', 'C', '3', 'N', '#', '2', 'F', ')', '1', '(', '5', '=', 'O', ' ']\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "folder_path = \"./datasets/\"\n",
    "file_name = \"SelectedSMILES_QM9.txt\"\n",
    "\n",
    "full_path = folder_path + file_name\n",
    "\n",
    "selfies_list, selfies_alphabet, largest_selfies_len, smiles_list, smiles_alphabet, largest_smiles_len = chemistry_vae_selfies.get_selfie_and_smiles_encodings_for_dataset(full_path)\n",
    "\n",
    "selfies_alphabet = ['[#Branch2]', '[Ring2]', '[Branch2]', '[=Branch2]', '[O]', '[=O]', '[=C]', '[=N]', '[#Branch1]', '[=Branch1]', '[nop]', '[N]', '[Branch1]', '[F]', '[#C]', '[#N]', '[Ring1]', '[C]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define source file location\n",
    "file_to_load =  \"./saved_models_RNN/\"\n",
    "# training file name encoder\n",
    "training_file_nameE = \"300/E\"\n",
    "# training file name decoder\n",
    "training_file_nameD = \"300/D\"\n",
    "# load data\n",
    "#load_data_trained = file_to_load + training_file_nameE\n",
    "# Alphabet has 18 letters, largest molecule is 21 letters. (build this as an output function later ... )\n",
    "largest_selfies_len_dataset = largest_selfies_len\n",
    "largest_smiles_len_dataset = largest_smiles_len\n",
    "\n",
    "#in_dimension = len(selfies_alphabet)*largest_selfies_len\n",
    "in_dimension = len(smiles_alphabet)*largest_smiles_len\n",
    "\n",
    "# load the trained encoder\n",
    "vae_encoder = torch.load(file_to_load + training_file_nameE) #, map_location=torch.device(device=\"cpu\"))\n",
    "#print(vae_encoder)\n",
    "\n",
    "# load the trained decoder\n",
    "vae_decoder = torch.load(file_to_load + training_file_nameD) #, map_location=torch.device(device=\"cpu\"))\n",
    "#print(vae_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Translating SMILES to SELFIES...\n",
      "Finished translating SMILES to SELFIES.\n",
      "selfies aplhabet: ['[#Branch2]', '[=C]', '[#C]', '[O]', '[Branch2]', '[#N]', '[#Branch1]', '[nop]', '[N]', '[=Branch1]', '[Ring1]', '[Ring2]', '[=O]', '[F]', '[C]', '[=N]', '[=Branch2]', '[Branch1]']\n",
      "smiles aplhabet: ['4', 'C', '3', 'N', '#', '2', 'F', ')', '1', '(', '5', '=', 'O', ' ']\n"
     ]
    }
   ],
   "source": [
    "selfies_rep, latent_rep, props_used = properties_searcher.filter(list_of_properties, largest_selfies_len_dataset, selfies_alphabet, vae_encoder, vae_decoder)\n",
    "\n",
    "selfies_list, selfies_alphabet, largest_selfies_len, smiles_list, smiles_alphabet, largest_smiles_len = chemistry_vae_selfies.get_selfie_and_smiles_encodings_for_dataset(full_path)\n",
    "\n",
    "\n",
    "selfies_alphabet = ['[#Branch2]', '[Ring2]', '[Branch2]', '[=Branch2]', '[O]', '[=O]', '[=C]', '[=N]', '[#Branch1]', '[=Branch1]', '[nop]', '[N]', '[Branch1]', '[F]', '[#C]', '[#N]', '[Ring1]', '[C]']\n",
    "train_size = round(len(latent_rep)*0.8)\n",
    "\n",
    "one_hots =  properties_searcher.create_onehot_instance_many(selfies_rep, largest_selfies_len, selfies_alphabet)\n",
    "\n",
    "energy_gaps = []\n",
    "#start_time = time.time()                                                                                                                                                                               \n",
    "\n",
    "\n",
    "latent_rep, log_var_rep = properties_searcher.gen_new_latent_space(one_hots, largest_selfies_len, selfies_alphabet, vae_encoder, vae_decoder)\n",
    "\n",
    "\n",
    "\n",
    "latent_space_vectors_valid = [item[0].detach().squeeze(0) for item in latent_rep]\n",
    "log_vars_valid = [item[0].detach().squeeze(0) for item in log_var_rep]\n",
    "properties_training = [torch.tensor(property_vector) for property_vector in props_used]  # this will convert lists to tensors  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130251\n",
      "130251\n"
     ]
    }
   ],
   "source": [
    "print(len(latent_space_vectors_valid))\n",
    "print(len(log_vars_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_normalize(tensor):\n",
    "    min_val = torch.min(tensor)\n",
    "    max_val = torch.max(tensor)\n",
    "    normalized_tensor = (tensor - min_val) / (max_val - min_val)\n",
    "    return normalized_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(properties_training)):\n",
    "    energy_gaps.append(props_used[i][0])\n",
    "\n",
    "\n",
    "\n",
    "log_var2 = [math.e ** (-exponent) for exponent in log_vars_valid]\n",
    "\n",
    "\n",
    "#log_vars_valid_normalise = (log_var2 - log_var2[(log_var2).argmin()]) / (log_var2[(log_var2).argmax()] - log_var2[(log_var2).argmin()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_tensor_list = [min_max_normalize(tensor) for tensor in log_var2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  870.1587,   541.6920,  1232.6119,  1406.4443,  3066.4688,  3538.2463,\n",
       "          480.1863,  1702.2864,  3667.7627,   695.9597,  2861.7271,  1762.9312,\n",
       "         4839.9648,  2551.9009,  3155.3718,  2501.1599,  1584.5022,  4314.9805,\n",
       "        11398.8779,  1621.9802,  1687.1516,   638.3410,  1715.5951,  1948.5055,\n",
       "         2966.8137])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_var2[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130251"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(normalized_tensor_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_var_rep_train = normalized_tensor_list[:train_size]\n",
    "log_var_rep_test = normalized_tensor_list[train_size:]\n",
    "\n",
    "\n",
    "\n",
    "latents_train = latent_space_vectors_valid[:train_size]\n",
    "latents_test = latent_space_vectors_valid[train_size:]\n",
    "\n",
    "props_train = energy_gaps[:train_size]\n",
    "props_test = energy_gaps[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([104201, 25])\n",
      "torch.Size([104201, 25])\n"
     ]
    }
   ],
   "source": [
    "train_latent_space_tensor, test_latent_space_tensor, train_properties_tensor, test_properties_tensor = properties_searcher.make_tensors(latents_train, latents_test, props_train, props_test)\n",
    "\n",
    "log_var_rep_train = torch.stack(log_var_rep_train)\n",
    "log_var_rep_test = torch.stack(log_var_rep_test)\n",
    "\n",
    "train_latent_space_tensor = train_latent_space_tensor.view(-1, input_dim)\n",
    "test_latent_space_tensor = test_latent_space_tensor.view(-1, input_dim)\n",
    "log_var_rep_train_tensor = log_var_rep_train.view(-1, input_dim)\n",
    "log_var_rep_test_tensor = log_var_rep_test.view(-1, input_dim)\n",
    "\n",
    "\n",
    "print(log_var_rep_train_tensor.size())\n",
    "print(train_latent_space_tensor.size())\n",
    "\n",
    "train_data_with_variances = torch.cat((train_latent_space_tensor, log_var_rep_train_tensor), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0255, 0.0000, 0.6345, 0.1347, 0.1470, 0.2773, 0.0462, 0.2967, 0.3003,\n",
      "        0.1171, 1.0000, 0.5250, 0.5243, 0.2843, 0.6753, 0.4109, 0.1829, 0.5580,\n",
      "        0.0815, 0.0795, 0.2203, 0.0371, 0.1314, 0.2583, 0.6376])\n"
     ]
    }
   ],
   "source": [
    "print(log_var_rep_test_tensor[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104201\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data_with_variances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104201\n",
      "104201\n"
     ]
    }
   ],
   "source": [
    "print(len(log_var_rep_train_tensor))\n",
    "print(len(train_latent_space_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104201\n"
     ]
    }
   ],
   "source": [
    "print(len(log_var_rep_train_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'properties_searcher' from '/users/sgccook3/DM_chems/VAE_sandbox/updated_juri/properties_searcher.py'>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(properties_searcher)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([104201, 512])\n",
      "torch.Size([15000, 512])\n",
      "torch.Size([15000, 512])\n",
      "torch.Size([15000, 512])\n",
      "torch.Size([15000, 512])\n",
      "torch.Size([15000, 512])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-cd21c2b42edd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvar_batch\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Pass the combined tensor as input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprops_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venv/lib64/python3.6/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venv/lib64/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(num_of_cycles):\n",
    "            # Sample input dimensions for 100,000 compounds represented by 25 numbers\n",
    "        hidden_dim = 1024 #random.choice([512, 1024, 2048])  # You can increase the hidden dimensions for more complexity\n",
    "        prop_hidden_dim = int(hidden_dim/2)  # You can adjust this based on the complexity of the task\n",
    "        prop_pred_activation = 'leaky_relu' #random.choice(['relu', 'leaky_relu'])\n",
    "        prop_pred_dropout = 0#random.choice([0, 0.1, 0.2])\n",
    "        prop_pred_depth = 6#random.choice([4,5,6])\n",
    "        prop_growth_factor = 0.4#random.choice([0.4, 0.5, 0.6])\n",
    "        lr_random = 0.0005#random.choice([0.001, 0.0005, 0.0001])\n",
    "        epochs = 2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Instantiate the model with hyperparameters\n",
    "        model = properties_searcher.PropertyRegressionModel(input_dim, hidden_dim, prop_hidden_dim, prop_pred_activation, prop_pred_dropout, prop_pred_depth, prop_growth_factor)\n",
    "\n",
    "        # Define your loss function (e.g., Mean Squared Error)\n",
    "        loss_function = nn.MSELoss()\n",
    "\n",
    "        # Define your optimizer (e.g., Adam)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr_random)   \n",
    "        decay_step_size = 100  # Adjust this as per your preference\n",
    "        decay_gamma = 0.9     # Adjust this as per your preference\n",
    "        scheduler = properties_searcher.StepLR(optimizer, step_size=decay_step_size, gamma=decay_gamma)\n",
    "\n",
    "\n",
    "        batch_size = 15000  # You can adjust this based on your hardware and dataset size\n",
    "\n",
    "        # Split the training data and properties into batches\n",
    "        train_data_batches = train_latent_space_tensor.split(batch_size)\n",
    "        train_props_batches = train_properties_tensor.split(batch_size)\n",
    "        latent_variances_batch = log_var_rep_train_tensor.split(batch_size)\n",
    "        print(len(latent_variances_batch[2][1]))\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "###\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(train_latent_space_tensor, log_var_rep_train_tensor)\n",
    "            loss = loss_function(predictions.squeeze(), train_properties_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            #end_time = time.time()\n",
    "            #epoch_duration = end_time - start_time\n",
    "\n",
    "            #print(f\"Epoch {epoch}/{epochs} - Duration: {epoch_duration:.2f} seconds\")\n",
    "\n",
    "            total_loss = 0.0\n",
    "\n",
    "            # Iterate through batches\n",
    "            for data_batch, props_batch, var_batch in zip(train_data_batches, train_props_batches, latent_variances_batch):\n",
    "\n",
    "                # Concatenate the latent space tensor and variances tensor for this batch\n",
    "                data_with_variances_batch = torch.cat((data_batch, var_batch), dim=1)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                predictions = model(data_batch,var_batch)  # Pass the combined tensor as input\n",
    "                loss = loss_function(predictions.squeeze(), props_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                \n",
    "\n",
    "                # Update the learning rate\n",
    "            scheduler.step()\n",
    "\n",
    "                # Calculate average loss for the epoch\n",
    "            avg_loss = total_loss / len(train_data_batches)\n",
    "\n",
    "\n",
    "\n",
    "###\n",
    "\n",
    "        with torch.no_grad():\n",
    "            test_predictions = model(test_latent_space_tensor, log_var_rep_test_tensor)\n",
    "            #print('test_predictions length:',len(test_predictions))\n",
    "            #print('actual predictions length:',len(props_test))\n",
    "            #test_loss = loss_function(test_predictions.squeeze(), test_properties_tensor)\n",
    "            #print(f\"Test Loss: {test_loss.item():.4f}\")\n",
    "\n",
    "###\n",
    "\n",
    "\n",
    "        # Generate predictions for the test set\n",
    "        y_pred = test_predictions\n",
    "        y_test = props_test\n",
    "\n",
    "        mse, mae, r2 = properties_searcher.stats(y_pred, y_test)\n",
    "        print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26050\n"
     ]
    }
   ],
   "source": [
    "print(len(test_latent_space_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'log_var_rep_train_tensor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6af3f0236179>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlog_var_rep_train_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'log_var_rep_train_tensor' is not defined"
     ]
    }
   ],
   "source": [
    "log_var_rep_train_tensor.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
