import os
import sys


import numpy as np
import selfies as sf
import pandas as pd
import torch
import yaml
from rdkit import rdBase
from torch import nn
from torch.optim.lr_scheduler import ReduceLROnPlateau
from torch.nn.utils.rnn import pad_sequence


def selfies_to_one_hot(encoding_list, encoding_alphabet, largest_molecule_len):

    '''One hot generation'''

    '''Arguments:
                    encoding_list: a list containing the SELFIES (list)
                    encoding_alphabet: the alphabet generated by the function get_selfie_and_smiles_encodings_for_dataset (list)
                    largest_molecule_len: the maximum length of the molecule encodings (int)'''
    
    '''Outputs:
                    one_hot: pytorch tensor of representing the SELFIES contained within the encoding_list (Pytorch float.32 tensor) '''

    alphabet_dict = {letter: index for index, letter in enumerate(encoding_alphabet)}


    integer_encoded = [[alphabet_dict[symbol] for symbol in sf.split_selfies(encoding_list[x])] for x in range(len(encoding_list))]
    max_length = max(len(inner_list) for inner_list in integer_encoded)
    padded_list = [torch.tensor(inner_list + [0] * (max_length - len(inner_list))) for inner_list in integer_encoded]
    padded_tensor = pad_sequence(padded_list, batch_first=True, padding_value=0)


    if padded_tensor.shape[1] < largest_molecule_len:
        extra_padding = torch.zeros(padded_tensor.shape[0],(largest_molecule_len - padded_tensor.shape[1]), dtype = torch.int64)
        padded_tensor = torch.cat((padded_tensor, extra_padding),dim=1)
    


    one_hot = torch.nn.functional.one_hot(padded_tensor, num_classes = len(encoding_alphabet))

    return one_hot



def _make_dir(directory):

    '''Makes the directory'''

    '''Arguments:
                    directory: directory path (str)'''
    os.makedirs(directory)


def get_free_memory(device):

    '''Calculate the amount of free memory'''

    '''Arguments:
                    device: thr device being used (Pytorch object)'''
    
    '''Outputs:
                    free_mem: the amount of free memory in bytes (float)'''


    memory_stats = torch.cuda.memory_stats(device)
    total_memory = memory_stats["allocated_bytes.all.peak"]
    memory_allocated = memory_stats["allocated_bytes.all.current"]
    free_mem = total_memory - memory_allocated

    return free_mem

def get_selfie_and_smiles_encodings_for_dataset(file_path):

    '''Returns encoding, alphabet and length of largest molecule in SMILES and SELFIES, given a file containing SMILES molecules.'''

    '''Arguments:
                    file_path: .csv file with molecules. Column name containing the smiles must be 'smiles' (str)'''
    
    '''Outputs:
                    selfies_list: the SELFIES encoding of the SMILES molecules provided (list)
                    selfies_alphabet: the alphabet of the SELFIES encoding (list)
                    largest_selfies_len: the longest SELFIES encoding length (int)
                    smiles_list: a list of the SMILES encodings (list)
                    smiles_alphabet: the alphabet of the SMILES encoding (list)
                    largest_smiles_len: the longest SMILES encoding length (int)'''
                    


    df = pd.read_csv(file_path)
    df = df.dropna()


    new_constraints = sf.get_semantic_constraints()
    new_constraints['N'] = 5
    new_constraints['B'] = 4

    sf.set_semantic_constraints(new_constraints)  # update constraints


    smiles_list = np.asanyarray(df.smiles)

    smiles_list = remove_unrecognized_symbols(smiles_list)


    print('--> Translating SMILES to SELFIES...')
    selfies_list = list(map(sf.encoder, smiles_list))


    all_selfies_symbols = sf.get_alphabet_from_selfies(selfies_list)
    selfies_alphabet = list(all_selfies_symbols)
    selfies_alphabet.insert(0, '[nop]')
    #selfies_alphabet.insert(1, '.')

    largest_selfies_len = max(sf.len_selfies(s) for s in selfies_list)

    print('Finished translating SMILES to SELFIES.')

    return selfies_list, selfies_alphabet, largest_selfies_len


def remove_unrecognized_symbols(smiles_list):

    '''Removes blank spaces from the SMILES encodings'''

    '''Arguments:
                    smiles_list: the list of SMILES encodings (list)'''
    
    '''Outputs:
                    cleaned_smiles: the cleaned SMILES encodings list (list)'''

    cleaned_smiles = [smiles.replace('\n', '') for smiles in smiles_list]

    return cleaned_smiles


