{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import selfies\n",
    "import torch\n",
    "import chemistry_vae_selfies\n",
    "import data_loader\n",
    "import chemestry_perceptron as cp\n",
    "import torch.nn as nn\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_file = pd.read_csv('./datasets/QM9listprops.csv', index_col=None)##The file you want to train on, should contain SMILES reps, latent space reps and properties\n",
    "my_file.dropna()\n",
    "\n",
    "list_of_properties = []\n",
    "\n",
    "\n",
    "for i in range(len(my_file)):\n",
    "\n",
    "    DipoleIn = my_file['dipole_moment'][i]\n",
    "    GapIn = my_file['energy_gap'][i]\n",
    "    SMILESCodeIn = my_file['smiles'][i]\n",
    "    list_of_properties.append([SMILESCodeIn, GapIn, DipoleIn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Translating SMILES to SELFIES...\n",
      "Finished translating SMILES to SELFIES.\n",
      "selfies aplhabet: ['[Branch2]', '[N]', '[=O]', '[=C]', '[Ring1]', '[Branch1]', '[Ring2]', '[#C]', '[O]', '[=N]', '[=Branch1]', '[#Branch1]', '[=Branch2]', '[nop]', '[F]', '[#N]', '[#Branch2]', '[C]']\n",
      "smiles aplhabet: ['4', '2', 'F', '=', '1', 'C', ')', '3', '5', '#', 'N', '(', 'O', ' ']\n"
     ]
    }
   ],
   "source": [
    "#Â load the encoder and decoder models from the VAE training\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "folder_path = \"./datasets/\"\n",
    "file_name = \"SelectedSMILES_QM9.txt\"\n",
    "\n",
    "full_path = folder_path + file_name\n",
    "\n",
    "selfies_list, selfies_alphabet, largest_selfies_len, smiles_list, smiles_alphabet, largest_smiles_len = chemistry_vae_selfies.get_selfie_and_smiles_encodings_for_dataset(full_path)\n",
    "\n",
    "#selfies_alphabet = ['[#Branch2]', '[Ring2]', '[Branch2]', '[=Branch2]', '[O]', '[=O]', '[=C]', '[=N]', '[#Branch1]', '[=Branch1]', '[nop]', '[N]', '[Branch1]', '[F]', '[#C]', '[#N]', '[Ring1]', '[C]']\n",
    "\n",
    "###\n",
    "\n",
    "    # define source file location\n",
    "file_to_load =  \"./saved_models_RNN/\"\n",
    "    # training file name encoder\n",
    "training_file_nameE = \"300/E\"\n",
    "    # training file name decoder\n",
    "training_file_nameD = \"300/D\"\n",
    "    # load data\n",
    "    #load_data_trained = file_to_load + training_file_nameE\n",
    "    # Alphabet has 18 letters, largest molecule is 21 letters. (build this as an output function later ... )\n",
    "largest_selfies_len_dataset = largest_selfies_len\n",
    "largest_smiles_len_dataset = largest_smiles_len\n",
    "\n",
    "    #in_dimension = len(selfies_alphabet)*largest_selfies_len\n",
    "in_dimension = len(smiles_alphabet)*largest_smiles_len\n",
    "\n",
    "    # load the trained encoder\n",
    "vae_encoder = torch.load(file_to_load + training_file_nameE) #, map_location=torch.device(device=\"cpu\"))\n",
    "    #print(vae_encoder)\n",
    "\n",
    "    # load the trained decoder\n",
    "vae_decoder = torch.load(file_to_load + training_file_nameD) #, map_location=torch.device(device=\"cpu\"))\n",
    "    #print(vae_decoder)\n",
    "\n",
    "\n",
    "selfies_alphabet = ['[#Branch2]', '[Ring2]', '[Branch2]', '[=Branch2]', '[O]', '[=O]', '[=C]', '[=N]', '[#Branch1]', '[=Branch1]', '[nop]', '[N]', '[Branch1]', '[F]', '[#C]', '[#N]', '[Ring1]', '[C]']\n",
    "\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n",
      "vector skipped\n"
     ]
    }
   ],
   "source": [
    "# select the molecules without pentabonds\n",
    "smiles_rep, latent_rep, props_used = cp.filter(list_of_properties, largest_selfies_len_dataset, selfies_alphabet, vae_encoder, vae_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to torch tensors and generate latent space vectors\n",
    "\n",
    "input_dim = 25 ### Size of the latent space\n",
    "\n",
    "latent_space_vectors_valid = [item[0].detach().squeeze(0) for item in latent_rep]\n",
    "properties_training = [torch.tensor(property_vector) for property_vector in props_used]  # this will convert lists to tensors\n",
    "\n",
    "train_size = round(len(latent_space_vectors_valid)*0.8)\n",
    "\n",
    "energy_gaps = []\n",
    "for i in range(len(properties_training)):\n",
    "    energy_gaps.append(props_used[i][0])\n",
    "\n",
    "latents_train = latent_space_vectors_valid[:train_size]\n",
    "latents_test = latent_space_vectors_valid[train_size:]\n",
    "\n",
    "props_train = energy_gaps[:train_size]\n",
    "props_test = energy_gaps[train_size:]\n",
    "\n",
    "train_latent_space_tensor, test_latent_space_tensor, train_properties_tensor, test_properties_tensor = cp.make_tensors(latents_train, latents_test, props_train, props_test)\n",
    "\n",
    "train_latent_space_tensor = train_latent_space_tensor.view(-1, input_dim)\n",
    "test_latent_space_tensor = test_latent_space_tensor.view(-1, input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Acquiring data...\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(\"settings_perceptron.yml\"):\n",
    "    settings = yaml.safe_load(open(\"settings_perceptron.yml\", \"r\"))\n",
    "else:\n",
    "    print(\"Expected a file settings_perceptron.yml but didn't find it.\")\n",
    "    \n",
    "print('--> Acquiring data...')\n",
    "# set the parameters manualy\n",
    "hidden_dim = settings['architecture']['hidden_dim'] \n",
    "prop_hidden_dim = settings['architecture']['prop_hidden_dim']\n",
    "prop_pred_activation =  settings['architecture']['prop_pred_activation']\n",
    "prop_pred_dropout =  settings['architecture']['prop_pred_dropout']\n",
    "prop_pred_depth = settings['architecture']['prop_pred_depth']\n",
    "prop_growth_factor =  settings['architecture']['prop_growth_factor']\n",
    "lr_input= settings['training']['lr']\n",
    "epochs = settings['training']['num_epochs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 62.1787\n",
      "Test Loss: 0.2090\n",
      "Test Loss: 0.0328\n",
      "Test Loss: 0.0499\n",
      "Test Loss: 0.0219\n",
      "Test Loss: 0.0117\n",
      "Test Loss: 0.0112\n",
      "Test Loss: 0.0063\n",
      "Test Loss: 0.0082\n",
      "Test Loss: 0.0110\n",
      "Test Loss: 0.0104\n",
      "Test Loss: 0.0078\n",
      "Test Loss: 0.0138\n",
      "Test Loss: 0.0098\n",
      "Test Loss: 0.0111\n",
      "Test Loss: 0.0123\n",
      "Test Loss: 0.0089\n",
      "Test Loss: 0.0122\n",
      "Test Loss: 0.0075\n",
      "Test Loss: 0.0101\n",
      "Test Loss: 0.0056\n",
      "Test Loss: 0.0077\n",
      "Test Loss: 0.0038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[E thread_pool.cpp:109] Exception in thread pool task: mutex lock failed: Invalid argument\n",
      "[E thread_pool.cpp:109] Exception in thread pool task: mutex lock failed: Invalid argument\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m predictions \u001b[39m=\u001b[39m model(train_latent_space_tensor)\n\u001b[1;32m     15\u001b[0m loss \u001b[39m=\u001b[39m loss_function(predictions\u001b[39m.\u001b[39msqueeze(), train_properties_tensor)\n\u001b[0;32m---> 16\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     17\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     21\u001b[0m \u001b[39m###\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Instantiate the model with hyperparameters\n",
    "model = cp.PropertyRegressionModel(input_dim, hidden_dim, prop_hidden_dim, prop_pred_activation, prop_pred_dropout, prop_pred_depth, prop_growth_factor)\n",
    "\n",
    "# Define your loss function (e.g., Mean Squared Error)\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "# Define your optimizer (e.g., Adam)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr_input)\n",
    "\n",
    "###\n",
    "\n",
    "for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(train_latent_space_tensor)\n",
    "        loss = loss_function(predictions.squeeze(), train_properties_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "        ###\n",
    "\n",
    "        with torch.no_grad():\n",
    "                test_predictions = model(test_latent_space_tensor)\n",
    "                test_loss = loss_function(test_predictions.squeeze(), test_properties_tensor)\n",
    "                print(f\"Test Loss: {test_loss.item():.4f}\")\n",
    "\n",
    "        ###\n",
    "\n",
    "\n",
    "        # Generate predictions for the test set\n",
    "        y_pred = test_predictions\n",
    "        y_test = props_test\n",
    "\n",
    "        mse, mae, r2 = cp.stats(y_pred, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.876308033040707"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
